{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":["52Tqm2OW-nDy","cvm2Fh61-yHq"],"toc_visible":true,"authorship_tag":"ABX9TyMUpNw/p4OFFiYh9xd1t2Z2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kQtIbvWVxkM0","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1603534624512,"user_tz":-120,"elapsed":5574,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"804ea30f-1731-4d1a-b5cd-3535f6c8415b"},"source":["#@markdown <b>Run me to import underscore module</b><br/>   {display-mode: \"form\"}\n","#@markdown <small>Method signatures:</small><br/> \n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _(source_path, target_path)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _set_gh_token(token)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _from_gh(user_name, repo_name, release_name) &nbsp; &nbsp; &nbsp; <b>Returns:</B> dictionary of arrays { 'array_name' : np.ndarray }</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs)</small></small><br/>\n","\n","!pip install -q githubrelease\n","import numpy as np\n","import os, glob, re, time\n","import github_release\n","\n","\n","def _compress(source_path, target_path, target_dir=None):\n","    if target_dir:\n","        !mkdir -p {target_dir}\n","    if target_path.endswith('.tar.gz'):\n","        !tar -czf {target_path} -C {source_path} .\n","    elif target_path.endswith('.tar'):\n","        !tar -cf {target_path} -C {source_path} .\n","    elif target_path.endswith('.zip'):\n","        !(cd {source_path} && zip -q -r {target_path} .)\n","\n","\n","def _extract(source_path, target_path):\n","    !mkdir -p {target_path}\n","    if source_path.endswith('.tar.gz'):\n","        !tar -xzf {source_path} -C {target_path}\n","    elif source_path.endswith('.tar'):\n","        !tar -xf {source_path} -C {target_path}\n","    elif source_path.endswith('.zip'):\n","        !unzip -qq {source_path} -d {target_path}\n","\n","\n","def _(source_path, target_path):\n","    \"\"\"\n","    Use cases:\n","        Movement:\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Compression (e.g. from dir to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction (e.g. from .zip to dir):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction & compression (e.g. from .zip to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","    \"\"\"\n","    COMPRESSION_FORMATS = ('zip', 'tar', 'tar.gz')\n","    TEMP_DIR = \"/tmp_\"\n","    LOG_TEMPLATE = \"{}    from    {}    to    {}\"\n","\n","    # Source\n","    source_dir, _, source_name = source_path.rpartition('/')\n","    source_isgcs = source_path.startswith(\"gs://\")\n","    source_islocal = not source_isgcs\n","    source_isprefix, source_isfile, source_ext = source_name.partition('.')\n","    source_isdir = not source_isfile\n","    source_iscompression = source_ext in COMPRESSION_FORMATS\n","\n","    # Target\n","    target_dir, _, target_name = target_path.rpartition('/')\n","    target_isgcs = target_path.startswith(\"gs://\")\n","    target_islocal = not target_isgcs\n","    target_prefix, target_isfile, target_ext = target_name.partition('.')\n","    target_isdir = not target_isfile\n","    target_iscompression = target_ext in COMPRESSION_FORMATS\n","\n","    # Flags\n","    MOVE_ONLY = source_ext == target_ext\n","    GCS_ONLY = source_isgcs and target_isgcs\n","    RENAME = source_isprefix != target_prefix\n","    COMPRESSION = source_isdir and target_iscompression\n","    EXTRACTION = source_iscompression and target_isdir\n","    EXTRACTION_COMPRESSION = source_iscompression and target_iscompression and source_ext != target_ext\n","\n","    # Authenticate if writing to GCS\n","    if target_isgcs:\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","    # Assert that subdirectories exist if target is local\n","    if target_islocal:\n","        !mkdir -p {target_dir}\n","\n","    # Movement commands\n","    if MOVE_ONLY:\n","        # GCS -> GCS\n","        if source_isgcs and target_isgcs:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !gsutil -m -q mv {source_path} {target_path}\n","        \n","        # LOCAL -> LOCAL\n","        elif source_islocal and target_islocal:\n","            print(LOG_TEMPLATE.format(\"MOVING (1/1)\", source_path, target_path))\n","            !mv {source_path} {target_path}\n","        \n","        # GCS -> LOCAL\n","        elif source_isgcs and target_islocal:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING DIR (1/1)\", source_path, target_dir))\n","                !gsutil -m -q cp -r {source_path} {target_dir}\n","                if RENAME:\n","                    print(LOG_TEMPLATE.format(\"\\tRENAMING DIR\", source_isprefix, target_prefix))\n","                    !mv {target_dir}/{source_isprefix} {target_dir}/{target_prefix}\n","            else:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        \n","        # LOCAL -> GCS\n","        if source_islocal and target_isgcs:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"UPLOADING DIR (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                print(LOG_TEMPLATE.format(\"UPLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        return\n","\n","\n","    # Create directory for intermediate storage if required\n","    if source_isgcs or target_isgcs or EXTRACTION_COMPRESSION:\n","        !mkdir -p {TEMP_DIR}\n","    \n","\n","    # For remaining operations, download GCS source to temp and treat as local\n","    if source_isgcs:\n","        if source_isdir:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING DIR\", source_path, TEMP_DIR))\n","            !gsutil -m -q cp -r {source_path} {TEMP_DIR}\n","        else:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING FILE\", source_path, f\"{TEMP_DIR}/{source_name}\"))\n","            !gsutil -m -q cp {source_path} {TEMP_DIR}/{source_name}\n","        source_path = f\"{TEMP_DIR}/{source_name}\"\n","        source_dir = TEMP_DIR\n","\n","    # Compression\n","    if COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/1)\", source_path, target_path))\n","            _compress(source_path, target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction\n","    elif EXTRACTION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/1)\", source_path, target_path))\n","            _extract(source_path, target_path)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING DIR (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp -r {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction & compression\n","    elif EXTRACTION_COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/2)\", f\"{TEMP_DIR}/{target_prefix}\", target_path))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/3)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/3)\", f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (3/3)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","    \n","    # Cleanup intermediate storage\n","    !rm -rf {TEMP_DIR}\n","\n","\n","def _set_gh_token(token):\n","    os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","def _export_array(array, release_name, prefix=\"\", splits=3):\n","    dir_path = f\"/tmp_/{release_name}\"\n","    !mkdir -p {dir_path}\n","    n_digits = len(str(splits - 1))\n","    subarrays = np.array_split(array, splits)\n","    for i, subarray in enumerate(subarrays):\n","        filename = f\"{prefix}__{str(i).zfill(n_digits)}.npy\"\n","        np.save(f\"{dir_path}/{filename}\", subarray)\n","\n","\n","def _concat_arrays(paths):\n","    return np.concatenate([np.load(path, allow_pickle=True) for path in sorted(paths)])\n","\n","\n","def _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs):\n","    # Assert that GitHub Auth token is set\n","    if \"GITHUB_TOKEN\" not in os.environ:\n","        print(\"GitHub authentication token is not set.\")\n","        print(\"Set token using the '_set_gh_token(token_string)' method.\")\n","        print(\"Minimal required auth scope is 'repo/public_repo' for public repositories.\")\n","        print(\"URL: https://github.com/settings/tokens/new\")\n","        return\n","\n","    # Split arrays\n","    for prefix, array in arr_kwargs.items():\n","        splits = int((array.nbytes/1_000_000) // split_size) + 1\n","        _export_array(array, release_name, prefix=prefix, splits=splits)\n","\n","    # Upload arrays\n","    github_release.gh_release_create(\n","        f\"{user_name}/{repo_name}\", \n","        release_name, \n","        publish=True, \n","        name=release_name, \n","        asset_pattern=f\"/tmp_/{release_name}/*\"\n","    )\n","    !rm -rf /tmp_/*\n","\n","\n","def _from_gh(user_name, repo_name, release_name):\n","    # Download release to temporary directory\n","    print(\"Downloading dataset in parallell ... \", end='\\t')\n","    t0 = time.perf_counter()\n","    assets = github_release.get_assets(f\"{user_name}/{repo_name}\", tag_name=release_name)\n","    download_urls = [asset['browser_download_url'] for asset in assets]\n","    urls_str = \" \".join(download_urls)\n","    !echo {urls_str} | xargs -n 1 -P 8 wget -q -P /tmp_/{release_name}_dl/\n","    t1 = time.perf_counter()\n","    print(f\"done! ({t1 - t0:.3f} seconds)\")\n","\n","    # Load data into numpy arrays\n","    paths = glob.glob(f\"/tmp_/{release_name}_dl/*.npy\")\n","    groups = {}\n","    for path in paths:\n","        match = re.match(r\".*/(.*)__[0-9]*\\.npy\", path)\n","        if match:\n","            prefix = match.group(1)\n","            groups[prefix] = groups.get(prefix, []) + [path]\n","    arrays_dict = {name: _concat_arrays(paths) for name, paths in groups.items()}\n","    !rm -rf /tmp_/*\n","    return arrays_dict"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Building wheel for linkheader (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"52Tqm2OW-nDy"},"source":["##### Initial setup"]},{"cell_type":"code","metadata":{"id":"S_zmL32x4Bpg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604572987913,"user_tz":-60,"elapsed":1464,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"08b13012-7621-42d9-8b3f-5bf77d640e8e"},"source":["# Download git repository\n","!git config --global user.email \"patrikkja@gmail.com\"\n","!git config --global user.name \"Patrik Kj√¶rran\"\n","!git clone -q https://github.com/patrikkj/marvin-models.git\n","%cd marvin-models"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/marvin-models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lApr2AmG3-Gw","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1603534688792,"user_tz":-120,"elapsed":58698,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"b3c11926-0410-4d04-ff2e-0d9bc1ebf2ce"},"source":["# Extract dataset into git/data directory\n","DATASET = \"dataset_full\"\n","_(f\"gs://marvin-voice/data/raw/{DATASET}.tar.gz\", f\"/content/marvin-models/data\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\tDOWNLOADING FILE    from    gs://marvin-voice/data/raw/dataset_full.tar.gz    to    /tmp_/dataset_full.tar.gz\n","EXTRACTING (1/1)    from    /tmp_/dataset_full.tar.gz    to    /content/marvin-models/data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XN-_a5jB4Y6G","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1603534706908,"user_tz":-120,"elapsed":8040,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"061aac88-4d3e-4be9-c22c-ef879c06a84b"},"source":["# Internal modules\n","import os, sys, glob\n","from importlib import reload\n","\n","# External modules\n","!pip install -q tensorflow-io\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorflow_io as tfio\n","from tensorflow_io import experimental as tfex\n","\n","# Colab modules\n","from google.colab import auth\n","from IPython import display\n","    \n","# Scripts\n","from scripts import preprocessing"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.4MB 1.3MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cvm2Fh61-yHq"},"source":["##### Create and export numpy arrys for each label"]},{"cell_type":"code","metadata":{"id":"-z8IlH_I_Ocr"},"source":["def generate_label_paths(data_dir):\n","    for dir_path in glob.iglob(DATA_DIR + \"/*\"):\n","        # Traverse directories only\n","        if not os.path.isdir(dir_path):\n","            continue\n","        \n","        # Fetch label\n","        label = dir_path.split('/')[-1]\n","\n","        # Ignore _background_noise_ directory\n","        if label.startswith(\"_\"):\n","            continue\n","        yield (dir_path, label)\n","\n","params = {'sample_rate': 16_000, 'min_freq': 0, 'max_freq': 8_000}\n","hparams = {\n","    'frame_size': 512,\n","    'frame_step': 256,\n","    'fft_size': 512,\n","    'mel_bins': 64,\n","\n","    'num_mfccs': 26,\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsCmzTEd-dtU","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603535045046,"user_tz":-120,"elapsed":325704,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"34c604a0-bff3-4f7a-9099-5c5364b49d5a"},"source":["from scripts import preprocessing, layers\n","\n","spectrogram_layer = layers.Spectrogram(params, hparams)\n","mel_spectrogram_layer = layers.MelSpectrogram(params, hparams)\n","log_mel_spectrogram_layer = layers.LogMelSpectrogram(params, hparams)\n","mfccs_layer = layers.MFCC(params, hparams)\n","\n","DATA_DIR = \"/content/marvin-models/data\"\n","!mkdir -p /tmp/arrays/tensors /tmp/arrays/log_mel_specs /tmp/arrays/mfccs\n","\n","for dir_path, label in list(generate_label_paths(DATA_DIR)):\n","    print(f\"Processing: {label}\\t{dir_path}\")\n","    paths = glob.glob(dir_path + \"/*.wav\")\n","\n","    print(\"Creating tensors ...\")\n","    tensors = tf.stack([preprocessing.to_tensor(path) for path in paths])\n","    np.save(f'/tmp/arrays/tensors/{label}', tensors)\n","\n","    print(\"Creating log_mel_specs ...\")\n","    spectrograms = spectrogram_layer(tensors)\n","    mel_spectrograms = mel_spectrogram_layer(spectrograms)\n","    log_mel_specs = log_mel_spectrogram_layer(mel_spectrograms)\n","    np.save(f'/tmp/arrays/log_mel_specs/{label}', log_mel_specs)\n","\n","    print(\"Creating mfccs ...\")\n","    mfccs = mfccs_layer(log_mel_specs)\n","    np.save(f'/tmp/arrays/mfccs/{label}', mfccs)\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing: marvin\t/content/marvin-models/data/marvin\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: zero\t/content/marvin-models/data/zero\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: six\t/content/marvin-models/data/six\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: seven\t/content/marvin-models/data/seven\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: two\t/content/marvin-models/data/two\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: one\t/content/marvin-models/data/one\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: off\t/content/marvin-models/data/off\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: three\t/content/marvin-models/data/three\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: four\t/content/marvin-models/data/four\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: left\t/content/marvin-models/data/left\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: bird\t/content/marvin-models/data/bird\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: stop\t/content/marvin-models/data/stop\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: tree\t/content/marvin-models/data/tree\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: nine\t/content/marvin-models/data/nine\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: dog\t/content/marvin-models/data/dog\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: down\t/content/marvin-models/data/down\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: wow\t/content/marvin-models/data/wow\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: yes\t/content/marvin-models/data/yes\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: sheila\t/content/marvin-models/data/sheila\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: bed\t/content/marvin-models/data/bed\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: happy\t/content/marvin-models/data/happy\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: house\t/content/marvin-models/data/house\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: cat\t/content/marvin-models/data/cat\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: right\t/content/marvin-models/data/right\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: no\t/content/marvin-models/data/no\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: go\t/content/marvin-models/data/go\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: five\t/content/marvin-models/data/five\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: eight\t/content/marvin-models/data/eight\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: on\t/content/marvin-models/data/on\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n","Processing: up\t/content/marvin-models/data/up\n","Creating tensors ...\n","Creating log_mel_specs ...\n","Creating mfccs ...\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L6wgDGD0h41M","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1603537053773,"user_tz":-120,"elapsed":256386,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"17621156606987172084"}},"outputId":"bc8e17d9-b890-4c83-a36e-bc849c9c94f3"},"source":["# Reimport incase VM ran out of memory\n","import numpy as np\n","import glob\n","_set_gh_token(\"73a1d93fa1e7fe7696321a86ff037a0ecc58346c\")\n","\n","for name in ('tensors', 'log_mel_specs', 'mfccs'):\n","    print(f\"Currently compressing: {name}\")\n","    # Free up memory from previous iteration\n","    data, labels = None, None\n","\n","    arrs, labels_arrs = [], []\n","    for npy_file in glob.glob(f\"/tmp/arrays/{name}/*.npy\"):\n","        arr = np.load(npy_file)\n","\n","        label = npy_file.split('/')[-1].split('.')[0]\n","        label_arr = np.full((arr.shape[0],), 1 if label == \"marvin\" else 0).astype(int)\n","\n","        arrs.append(arr)\n","        labels_arrs.append(label_arr)\n","\n","    data = np.vstack(arrs).squeeze()\n","    del arrs\n","\n","    labels = np.concatenate(labels_arrs).squeeze()\n","    del labels_arrs\n","\n","    m = data.shape[0]\n","    p = np.random.permutation(m)\n","    data, labels = data[p], labels[p]\n","    \n","    pos_data = data[labels==1, ...]\n","    pos_labels = labels[labels==1, ...]\n","\n","    neg_data = data[labels==0, ...][::2]\n","    neg_labels = labels[labels==0, ...][::2]\n","\n","    arr_kwargs = {\n","        \"pos_data\": pos_data,\n","        \"pos_labels\": pos_labels,\n","        \"neg_data\": neg_data,\n","        \"neg_labels\": neg_labels\n","    }\n","\n","    _to_gh(\"patrikkj\", \"marvin-models\", f\"dataset_half_notrim_{name}\", split_size=200, **arr_kwargs)\n","\n","    #np.savez(f'/tmp/{name}', data=data, labels=labels)\n","    #NEW_DATASET_NAME = \"dataset_full\"\n","    #_(f\"/tmp/{name}.npz\", f\"gs://marvin-voice/data/arrays/{NEW_DATASET_NAME}/{name}.npz\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Currently compressing: tensors\n","created 'dataset_half_notrim_tensors' release\n","  Tag name      : dataset_half_notrim_tensors\n","  Name          : dataset_half_notrim_tensors\n","  ID            : 33010663\n","  Created       : 2020-10-23T17:07:49Z\n","  URL           : https://github.com/patrikkj/marvin-models/releases/tag/dataset_half_notrim_tensors\n","  Author        : patrikkj\n","  Is published  : True\n","  Is prerelease : False\n","\n","uploading 'dataset_half_notrim_tensors' release asset(s) (found 14):\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__08.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__08.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__09.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__09.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__00.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__00.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__10.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__10.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__03.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__03.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/pos_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/pos_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__05.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__05.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__01.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__01.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__04.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__04.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/pos_data__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/pos_data__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__07.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__07.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__06.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__06.npy\n","\n","  uploading /tmp_/dataset_half_notrim_tensors/neg_data__02.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_tensors/neg_data__02.npy\n","\n","Currently compressing: log_mel_specs\n","created 'dataset_half_notrim_log_mel_specs' release\n","  Tag name      : dataset_half_notrim_log_mel_specs\n","  Name          : dataset_half_notrim_log_mel_specs\n","  ID            : 33010686\n","  Created       : 2020-10-23T17:07:49Z\n","  URL           : https://github.com/patrikkj/marvin-models/releases/tag/dataset_half_notrim_log_mel_specs\n","  Author        : patrikkj\n","  Is published  : True\n","  Is prerelease : False\n","\n","uploading 'dataset_half_notrim_log_mel_specs' release asset(s) (found 6):\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/neg_data__2.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/neg_data__2.npy\n","\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/neg_data__1.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/neg_data__1.npy\n","\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/pos_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/pos_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/neg_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/neg_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/pos_data__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/pos_data__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_log_mel_specs/neg_data__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_log_mel_specs/neg_data__0.npy\n","\n","Currently compressing: mfccs\n","created 'dataset_half_notrim_mfccs' release\n","  Tag name      : dataset_half_notrim_mfccs\n","  Name          : dataset_half_notrim_mfccs\n","  ID            : 33010696\n","  Created       : 2020-10-23T17:07:49Z\n","  URL           : https://github.com/patrikkj/marvin-models/releases/tag/dataset_half_notrim_mfccs\n","  Author        : patrikkj\n","  Is published  : True\n","  Is prerelease : False\n","\n","uploading 'dataset_half_notrim_mfccs' release asset(s) (found 4):\n","  uploading /tmp_/dataset_half_notrim_mfccs/pos_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_mfccs/pos_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_mfccs/neg_labels__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_mfccs/neg_labels__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_mfccs/pos_data__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_mfccs/pos_data__0.npy\n","\n","  uploading /tmp_/dataset_half_notrim_mfccs/neg_data__0.npy\n","  download_url: https://github.com/patrikkj/marvin-models/releases/download/dataset_half_notrim_mfccs/neg_data__0.npy\n","\n"],"name":"stdout"}]}]}