{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dataframes.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP6qaS2VIRfxRCMucgXTlKR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kQtIbvWVxkM0","cellView":"form"},"source":["#@markdown <b>Run me to import underscore module</b><br/>   {display-mode: \"form\"}\n","!pip install -q tensorflowjs\n","!pip install -q githubrelease\n","import numpy as np\n","import os, glob, re, time, json\n","import github_release\n","import tensorflow.keras.backend as K\n","from IPython import display\n","from contextlib import redirect_stdout\n","\n","compressed_dirs = set()\n","\n","\n","def _compress(source_path, target_path, target_dir=None):\n","    if target_dir:\n","        !mkdir -p {target_dir}\n","    if target_path.endswith('.tar.gz'):\n","        !tar -czf {target_path} -C {source_path} .\n","    elif target_path.endswith('.tar'):\n","        !tar -cf {target_path} -C {source_path} .\n","    elif target_path.endswith('.zip'):\n","        !(cd {source_path} && zip -q -r {target_path} .)\n","\n","\n","def _extract(source_path, target_path):\n","    !mkdir -p {target_path}\n","    if source_path.endswith('.tar.gz'):\n","        !tar -xzf {source_path} -C {target_path}\n","    elif source_path.endswith('.tar'):\n","        !tar -xf {source_path} -C {target_path}\n","    elif source_path.endswith('.zip'):\n","        !unzip -qq {source_path} -d {target_path}\n","\n","\n","def _under(source_path, target_path, copy=True, auth_on_upload=True):\n","    \"\"\"\n","    Use cases:\n","        Movement:\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Compression (e.g. from dir to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction (e.g. from .zip to dir):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction & compression (e.g. from .zip to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","    \"\"\"\n","    COMPRESSION_FORMATS = ('zip', 'tar', 'tar.gz')\n","    TEMP_DIR = \"/tmp_\"\n","    LOG_TEMPLATE = \"{}    from    {}    to    {}\"\n","\n","    # Source\n","    if source_path.endswith(\"/\"):\n","        source_path = source_path[:-1]\n","    source_dir, _, source_name = source_path.rpartition('/')\n","    source_isgcs = source_path.startswith(\"gs://\")\n","    source_islocal = not source_isgcs\n","    if source_islocal:\n","        source_path = os.path.abspath(source_path)\n","    source_isprefix, source_isfile, source_ext = source_name.partition('.')\n","    source_isdir = not source_isfile\n","    source_iscompression = source_ext in COMPRESSION_FORMATS\n","\n","    # Target\n","    target_dir, _, target_name = target_path.rpartition('/')\n","    target_isgcs = target_path.startswith(\"gs://\")\n","    target_islocal = not target_isgcs\n","    target_prefix, target_isfile, target_ext = target_name.partition('.')\n","    target_isdir = not target_isfile\n","    target_iscompression = target_ext in COMPRESSION_FORMATS\n","\n","    # Flags\n","    MOVE_ONLY = source_ext == target_ext\n","    GCS_ONLY = source_isgcs and target_isgcs\n","    RENAME = source_isprefix != target_prefix\n","    COMPRESSION = source_isdir and target_iscompression\n","    EXTRACTION = source_iscompression and target_isdir\n","    EXTRACTION_COMPRESSION = source_iscompression and target_iscompression and source_ext != target_ext\n","\n","    # Add forward slash if file is at root level\n","    source_dir = \"/\" if not source_dir else source_dir\n","    target_dir = \"/\" if not target_dir else target_dir\n","\n","    # Authenticate if writing to GCS\n","    if target_isgcs and auth_on_upload:\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","    # Assert that subdirectories exist if target is local\n","    if target_islocal:\n","        !mkdir -p {target_dir}\n","\n","    # Movement commands\n","    if MOVE_ONLY:\n","        # GCS -> GCS\n","        if source_isgcs and target_isgcs:\n","            action = \"COPYING\" if copy else \"MOVING\"\n","            print(LOG_TEMPLATE.format(f\"{action} (1/1)\", source_path, target_path))\n","            if copy:\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                !gsutil -m -q mv {source_path} {target_path}\n","        \n","        # LOCAL -> LOCAL\n","        elif source_islocal and target_islocal:\n","            action = \"COPYING\" if copy else \"MOVING\"\n","            print(LOG_TEMPLATE.format(f\"{action} (1/1)\", source_path, target_path))\n","            if copy:\n","                !cp -r {source_path} {target_path}\n","            else:\n","                !mv {source_path} {target_path}\n","        \n","        # GCS -> LOCAL\n","        elif source_isgcs and target_islocal:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING DIR (1/1)\", source_path, target_dir))\n","                !gsutil -m -q cp -r {source_path} {target_dir}\n","                if RENAME:\n","                    print(LOG_TEMPLATE.format(\"\\tRENAMING DIR\", source_isprefix, target_prefix))\n","                    !mv {target_dir}/{source_isprefix} {target_dir}/{target_prefix}\n","            else:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        \n","        # LOCAL -> GCS\n","        if source_islocal and target_isgcs:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"UPLOADING DIR (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                print(LOG_TEMPLATE.format(\"UPLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        return\n","\n","\n","    # Create directory for intermediate storage if required\n","    if source_isgcs or target_isgcs or EXTRACTION_COMPRESSION:\n","        !mkdir -p {TEMP_DIR}\n","    \n","\n","    # For remaining operations, download GCS source to temp and treat as local\n","    if source_isgcs:\n","        if source_isdir:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING DIR\", source_path, TEMP_DIR))\n","            !gsutil -m -q cp -r {source_path} {TEMP_DIR}\n","        else:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING FILE\", source_path, f\"{TEMP_DIR}/{source_name}\"))\n","            !gsutil -m -q cp {source_path} {TEMP_DIR}/{source_name}\n","        source_path = f\"{TEMP_DIR}/{source_name}\"\n","        source_dir = TEMP_DIR\n","\n","    # Compression\n","    if COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/1)\", source_path, target_path))\n","            _compress(source_path, target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction\n","    elif EXTRACTION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/1)\", source_path, target_path))\n","            _extract(source_path, target_path)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING DIR (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp -r {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction & compression\n","    elif EXTRACTION_COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/2)\", f\"{TEMP_DIR}/{target_prefix}\", target_path))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/3)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/3)\", f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (3/3)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","    \n","    # Cleanup intermediate storage\n","    !rm -rf {TEMP_DIR}\n","\n","def _set_gh_token(token):\n","    os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","def _export_array(array, release_name, prefix=\"\", splits=3):\n","    dir_path = f\"/tmp_/{release_name}\"\n","    !mkdir -p {dir_path}\n","    n_digits = len(str(splits - 1))\n","    subarrays = np.array_split(array, splits)\n","    for i, subarray in enumerate(subarrays):\n","        filename = f\"{prefix}__{str(i).zfill(n_digits)}.npy\"\n","        np.save(f\"{dir_path}/{filename}\", subarray)\n","\n","\n","def _concat_arrays(paths):\n","    return np.concatenate([np.load(path, allow_pickle=True) for path in sorted(paths)])\n","\n","\n","def _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs):\n","    # Assert that GitHub Auth token is set\n","    if \"GITHUB_TOKEN\" not in os.environ:\n","        print(\"GitHub authentication token is not set.\")\n","        print(\"Set token using the '_set_gh_token(token_string)' method.\")\n","        print(\"Minimal required auth scope is 'repo/public_repo' for public repositories.\")\n","        print(\"URL: https://github.com/settings/tokens/new\")\n","        return\n","\n","    # Split arrays\n","    for prefix, array in arr_kwargs.items():\n","        splits = int((array.nbytes/1_000_000) // split_size) + 1\n","        _export_array(array, release_name, prefix=prefix, splits=splits)\n","\n","    # Upload arrays\n","    github_release.gh_release_create(\n","        f\"{user_name}/{repo_name}\", \n","        release_name, \n","        publish=True, \n","        name=release_name, \n","        asset_pattern=f\"/tmp_/{release_name}/*\"\n","    )\n","    !rm -rf /tmp_/*\n","\n","\n","def _from_gh(user_name, repo_name, release_name):\n","    # Download release to temporary directory\n","    print(\"Downloading dataset in parallell ... \", end='\\t')\n","    t0 = time.perf_counter()\n","    assets = github_release.get_assets(f\"{user_name}/{repo_name}\", tag_name=release_name)\n","    download_urls = [asset['browser_download_url'] for asset in assets]\n","    urls_str = \" \".join(download_urls)\n","    !echo {urls_str} | xargs -n 1 -P 8 wget -q -P /tmp_/{release_name}_dl/\n","    t1 = time.perf_counter()\n","    print(f\"done! ({t1 - t0:.3f} seconds)\")\n","\n","    # Load data into numpy arrays\n","    paths = glob.glob(f\"/tmp_/{release_name}_dl/*.npy\")\n","    groups = {}\n","    for path in paths:\n","        match = re.match(r\".*/(.*)__[0-9]*\\.npy\", path)\n","        if match:\n","            prefix = match.group(1)\n","            groups[prefix] = groups.get(prefix, []) + [path]\n","    arrays_dict = {name: _concat_arrays(paths) for name, paths in groups.items()}\n","    !rm -rf /tmp_/*\n","    return arrays_dict\n","    \n","\n","def _log_to_gh(user, repo, tag, log_dir=\"/tmp/logs\"):\n","    # Create temporary directory for compressed logs\n","    !mkdir -p /tmp/compressed_logs\n","    \n","    # Compress all directories in log dir\n","    for dirname in os.listdir(log_dir):\n","        # Skip files\n","        if \".\" in dirname or dirname in compressed_dirs:\n","            continue\n","\n","        # Compress\n","        _under(f\"{log_dir}/{dirname}\", f\"/tmp/compressed_logs/{dirname}.tar.gz\")\n","        compressed_dirs.add(dirname)\n","\n","    # Upload compressed logs to GitHub\n","    github_release.gh_asset_upload(f\"{user}/{repo}\", tag, f\"/tmp/compressed_logs/*.tar.gz\")\n","\n","    # Cleanup compressed logs\n","    !rm -rf /tmp/compressed_logs/*\n","\n","def timeit(method):\n","    def timed(*args, **kw):\n","        ts = time.perf_counter()\n","        result = method(*args, **kw)\n","        te = time.perf_counter()\n","        diff = te - ts\n","        print(f\"{method.__name__}: {diff:.8f} s\")\n","        return result\n","    return timed\n","\n","class NpEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        else:\n","            return super(NpEncoder, self).default(obj)\n","\n","@timeit\n","def _export_model(model, model_name, model_type, val_dataset, test_dataset, params, hparams, history, log_dir, n_prep_layers=None):\n","    # Create temporary directory\n","    target_dir = f\"/tmp/models/{model_type}/{model_name}\"\n","    !mkdir -p {target_dir}     #/tmp_/models/rnn_naive/rnn_naive_20201108_130308\n","\n","    # Write export logs to file\n","    export_logs_path = os.path.join(target_dir, \"export_logs.txt\")\n","    with open(export_logs_path, 'w') as export_logs:\n","        with redirect_stdout(export_logs):\n","            # Profile model on inputs (average of n_runs cycles)\n","            n_runs = 5\n","            input_shape = val_dataset.element_spec[0].shape\n","\n","            def time_examples(model, n):\n","                dummy = np.random.rand(n, *input_shape[1:])\n","                t0 = time.perf_counter()\n","                model.predict(dummy)\n","                return time.perf_counter() - t0\n","\n","            with tf.device('/CPU:0'):\n","                cpu_profiles = {\n","                    \"cpu_1\": np.array([time_examples(model, 1) for _ in range(n_runs)]).mean(),\n","                    \"cpu_10\": np.array([time_examples(model, 10) for _ in range(n_runs)]).mean(),\n","                    \"cpu_100\": np.array([time_examples(model, 100) for _ in range(n_runs)]).mean()\n","                }\n","\n","            with tf.device('/GPU:0'):\n","                gpu_profiles = {\n","                    \"gpu_1\": np.array([time_examples(model, 1) for _ in range(n_runs)]).mean(),\n","                    \"gpu_10\": np.array([time_examples(model, 10) for _ in range(n_runs)]).mean(),\n","                    \"gpu_100\": np.array([time_examples(model, 100) for _ in range(n_runs)]).mean()\n","                }\n","\n","            # Get number of parameters\n","            params_counts = {\n","                \"trainable_params\": np.sum([K.count_params(w) for w in model.trainable_weights]),\n","                \"non_trainable_params\": np.sum([K.count_params(w) for w in model.non_trainable_weights])\n","            }\n","            params_counts[\"total_params\"] = params_counts[\"trainable_params\"] + params_counts[\"non_trainable_params\"]\n","\n","            # Generate evaluation metrics for validation and test set\n","            final_metrics_val = model.evaluate(val_dataset, return_dict=True)\n","            final_metrics_val = {f\"final_val_{k}\": v for k, v in final_metrics_val.items()}\n","            final_metrics_test = model.evaluate(test_dataset, return_dict=True)\n","            final_metrics_test = {f\"final_test_{k}\": v for k, v in final_metrics_test.items()}\n","\n","            # Generate Dataframe and export to parquet\n","            logs_params = {\n","                **params,\n","                **hparams,\n","                **history.params,\n","                **cpu_profiles,\n","                **gpu_profiles,\n","                **params_counts,\n","                **final_metrics_val,\n","                **final_metrics_test\n","            }\n","            logs_df = pd.DataFrame({**history.history, \"epoch\": history.epoch})\n","            for param, value in logs_params.items():\n","                logs_df[param] = value\n","            logs_df.to_parquet(os.path.join(target_dir, \"logs.parquet\"))\n","\n","            # Dump all parameters and metadata to .json file\n","            with open(os.path.join(target_dir, 'model_details.json'), 'w') as f:\n","                json.dump(logs_params, f, cls=NpEncoder, indent=4)\n","\n","            def _convert_model(model, subdir=\"model\"):\n","                # Create subdirectory\n","                subdir_path = os.path.join(target_dir, subdir)\n","                !mkdir -p {subdir_path}\n","\n","                # Write model summary to file\n","                model_summary_path = os.path.join(subdir_path, \"model_summary.txt\")\n","                with open(model_summary_path, 'w') as model_summary:\n","                    with redirect_stdout(model_summary):\n","                        model.summary()\n","\n","                # Export model summary as image\n","                model_summary_img_path = os.path.join(subdir_path, \"model_summary.png\")\n","                tf.keras.utils.plot_model(model, to_file=model_summary_img_path, show_shapes=True)\n","\n","                # Generate model paths\n","                keras_model_path = os.path.join(subdir_path, \"keras_model.h5\")\n","                saved_model_path = os.path.join(subdir_path, \"saved_model\")\n","                tfjs_layers_model_path = os.path.join(subdir_path, \"tfjs_layers_model\")\n","                tfjs_graph_model_path = os.path.join(subdir_path, \"tfjs_graph_model\")\n","\n","                # Save and convert model\n","                model.save(keras_model_path)\n","                tf.saved_model.save(model, saved_model_path)\n","                !tensorflowjs_converter --input_format=keras --output_format=tfjs_layers_model {keras_model_path} {tfjs_layers_model_path}\n","                !tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model {saved_model_path} {tfjs_graph_model_path}\n","            \n","            # Convert full model\n","            _convert_model(model, subdir=\"model\")\n","\n","            if n_prep_layers is not None:\n","                model_1 = tf.keras.Sequential(model.layers[:n_prep_layers])\n","                model_1.build(input_shape=input_shape)\n","                \n","                model_2 = tf.keras.Sequential(model.layers[n_prep_layers:])\n","                model_2.build(input_shape=model_1.layers[-1].output_shape)\n","\n","                # Convert models\n","                _convert_model(model_1, subdir=\"submodel_1\")\n","                _convert_model(model_2, subdir=\"submodel_2\")\n","\n","            # Compress TensorBoard logs\n","            model_log_dir = os.path.join(LOG_DIR, model_name)\n","            tensorboard_logs_path = os.path.join(target_dir, \"tensorboard.tar.gz\")\n","            _under(model_log_dir, tensorboard_logs_path)\n","\n","    # Upload logs to GCS\n","    _under(target_dir, f\"gs://marvin-voice/models/{model_type}/{model_name}\", auth_on_upload=False)\n","    return logs_df\n","\n","display.clear_output(wait=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2iS4I6NBs-7","cellView":"form"},"source":["#@markdown <b>Load dataframes</b><br/>   {display-mode: \"form\"}\n","import pandas as pd\n","# pd.set_option('display.max_rows', 500)\n","# pd.set_option('display.max_columns', 500)\n","# pd.set_option('display.width', 1000)\n","\n","def logs_to_df(model_type):\n","    # Download .parquet files\n","    target_dir = f\"/tmp/df_files/{model_type}\"\n","    !mkdir -p {target_dir}\n","    !gsutil -m cp gs://marvin-voice/models/{model_type}/**/*.parquet {target_dir}\n","\n","    # Load all .parquet files as dataframes\n","    dfs = []\n","    for path in glob.glob(f\"{target_dir}/**/*.parquet\", recursive=True):\n","        df = pd.read_parquet(path)\n","        model_name = path.split(os.sep)[-1].split('.')[0]\n","        df['model_name'] = model_name\n","        dfs.append(df)\n","    return pd.concat(dfs, ignore_index=True)\n","\n","rnn_df = logs_to_df(\"rnn_fft\")\n","cnn_df = logs_to_df(\"cnn_fft\")\n","rnn_naive_df = logs_to_df(\"rnn_naive\")\n","\n","display.clear_output(wait=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G95RPXxYPphm","cellView":"form"},"source":["#@markdown <b>Create unique dataframes</b><br/>   {display-mode: \"form\"}\n","\n","rnn_df_unique = rnn_df[rnn_df.epoch == 1]\n","cnn_df_unique = cnn_df[cnn_df.epoch == 1]\n","rnn_naive_df_unique = rnn_naive_df[rnn_naive_df.epoch == 1]\n","\n","unique_drop_cols = [\n","    \"train_split\", \"val_split\", \"test_split\", \"sample_rate\", \"min_freq\", \"max_freq\",\n","    \"loss\", \"precision\", \"recall\", \"accuracy\", \n","    \"val_loss\", \"val_precision\", \"val_recall\", \"val_accuracy\", \"epoch\",\n","    \"cpu_1\", \"cpu_10\", \"cpu_100\", \"gpu_1\", \"gpu_10\", \"gpu_100\",\n","]\n","rnn_df_unique = rnn_df_unique.drop(unique_drop_cols, axis=1)\n","cnn_df_unique = cnn_df_unique.drop(unique_drop_cols, axis=1)\n","rnn_naive_df_unique = rnn_naive_df_unique.drop(unique_drop_cols, axis=1)\n","\n","common_columns = rnn_df.columns & rnn_naive_df.columns & cnn_df.columns\n","common_df = pd.concat([rnn_df[common_columns], rnn_naive_df[common_columns], cnn_df[common_columns]])\n","common_df_unique = common_df[common_df.epoch == 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8Ip4wienEzC"},"source":["##### Show dataframes"]},{"cell_type":"code","metadata":{"id":"3I2HuHNzMAx0"},"source":["#@title RNN DataFrame { vertical-output: true, output-height: 100, display-mode: \"form\" }\n","rnn_df_unique.sort_values(\"final_val_accuracy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Q-crfTPUU2V"},"source":["#@title CNN Naive DataFrame { vertical-output: true, output-height: 100, display-mode: \"form\" }\n","cnn_df_unique.sort_values(\"final_val_accuracy\").tail(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QwSpohnUVJ3"},"source":["#@title RNN Naive DataFrame { vertical-output: true, output-height: 100, display-mode: \"form\" }\n","rnn_naive_df_unique.sort_values(\"final_val_accuracy\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yAq4LoihuuAu"},"source":["rnn_naive_df[[\"model_name\", \"epoch\"]].groupby(\"model_name\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuVaE-o6q_Hr"},"source":["final_rnn_naive = rnn_naive_df[(rnn_naive_df.model_name == 'rnn_naive_20201115_034239')]\n","final_cnn = cnn_df[(cnn_df.model_name == 'cnn_fft_20201111_063110')]\n","final_rnn = rnn_df[(rnn_df.model_name == 'rnn_fft_20201111_022641')]\n","final_df = pd.concat([final_rnn_naive, final_cnn, final_rnn], axis=0)\n","final_df\n","# final_df = rnn_naive_df.copy()\n","# final_df.append()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9LzapOe78oJ"},"source":["final_df.groupby(\"model_name\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMOf2VdKtIoE"},"source":["final_df_unique = final_df[final_df.epoch == 1]\n","final_df_unique.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3VAzg98tpLA"},"source":["columns = [\n","    'model_name',\n","    'dataset',\n","    '__timestamp__',\n","    'train_split',\n","    'val_split',\n","    'test_split',\n","    'sample_rate',\n","    'min_freq',\n","    'max_freq',\n","    'n_timesteps',\n","    'conv_filters',\n","    'conv_kernel_size',\n","    'conv_stride',\n","    'recurrent_cell',\n","    'recurrent_layers',\n","    'recurrent_units',\n","    'batch_size',\n","    'learning_rate',\n","    'optimizer',\n","    'verbose',\n","    'epochs',\n","    'steps',\n","    'cpu_1',\n","    'cpu_10',\n","    'cpu_100',\n","    'gpu_1',\n","    'gpu_10',\n","    'gpu_100',\n","    'trainable_params',\n","    'non_trainable_params',\n","    'total_params',\n","    'final_val_loss',\n","    'final_val_precision',\n","    'final_val_recall',\n","    'final_val_accuracy',\n","    'final_test_loss',\n","    'final_test_precision',\n","    'final_test_recall',\n","    'final_test_accuracy',\n","    'num_epochs',\n","    'frame_size',\n","    'frame_step',\n","    'fft_size',\n","    'mel_bins',\n","    'max_time_mask',\n","    'max_freq_mask',\n","    'conv_layers',\n","    'maxpool_size',\n","    'dense_layers',\n","    'dense_units',\n","    'dropout',\n","]\n","\n","params_df = final_df_unique[columns].T\n","params_df.to_latex(\"all_params.tex\", \n","            multirow=True, \n","            caption=\"All parameters and hyperparameters for the three models.\", \n","            float_format=lambda x: f\"{x:.3f}\" if np.mod(x, 1) else f\"{int(x)}\",\n","            bold_rows=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3I1LmQyu7Jt"},"source":["for col in final_df_unique.columns:\n","    print(f\"'{col}',\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BD0qXpIjL-XS"},"source":["#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n","#\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBqIbW0fEWGT"},"source":["rnn_df.groupby('pos_weight').mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6WXRUzf0sUrK"},"source":["filtered_df = rnn_df[rnn_df['__timestamp__'] >= 20201110000000]\n","filtered_df[filtered_df.epoch == 1][[\"model_name\", \"pos_weight\", \"frame_size\",\t\"frame_step\",\t\"fft_size\",\t\"mel_bins\", \"final_val_accuracy\", \"final_test_accuracy\", \"final_val_recall\", \"learning_rate\", \"cpu_1\", \"gpu_1\", \"recurrent_cell\"]].sort_values('final_val_accuracy').tail(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmxOkiK8dUWq"},"source":["rnn_df[rnn_df['__timestamp__'] >= 20201110000000]]\n","rnn_df[rnn_df.epoch == 1][[\"model_name\", \"frame_size\",\t\"frame_step\",\t\"fft_size\",\t\"mel_bins\", \"final_val_accuracy\", \"final_val_recall\", \"learning_rate\", \"cpu_1\", \"gpu_1\", \"recurrent_cell\"]].sort_values('final_val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8j8_-mVuxt8X"},"source":["rnn_naive_df[rnn_naive_df.epoch == 1][[\"model_name\", \"final_val_accuracy\", \"cpu_1\", \"gpu_1\", \"recurrent_cell\"]].sort_values('final_val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZdGIjT4DhT5"},"source":["rnn_naive_df[rnn_naive_df.epoch == 1].sort_values('final_val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Duk8gjwOZ6-J"},"source":["cnn_df[cnn_df.epoch == 1][[\"model_name\", \"frame_size\",\t\"frame_step\",\t\"fft_size\",\t\"mel_bins\", \"final_val_accuracy\", \"cpu_1\", \"gpu_1\"]].sort_values('final_val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dw3XPwULcfM9"},"source":["import seaborn as sns\n","sns.set(rc={'figure.figsize':(8,6)})\n","# grid = sns.lineplot(data=rnn_naive_df, x=\"epoch\", y=\"val_accuracy\", hue=\"model_name\")\n","grid = sns.lineplot(data=final_df, x=\"epoch\", y=\"val_accuracy\", hue=\"model_name\")\n","# grid.set(ylim=(0.8, None))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpIxhUtH5sKC"},"source":["common_df[common_df.epoch == 1][[\"model_name\", \"pos_weight\", \"final_val_accuracy\", \"final_val_recall\", \"final_val_precision\",  \"final_test_accuracy\", \"final_test_recall\", \"final_test_precision\", \"cpu_1\", \"gpu_1\"]].sort_values('final_val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vs8HFjay1vgk"},"source":["common_columns = rnn_df.columns & cnn_df.columns\n","common_df = pd.concat([rnn_df[common_columns], cnn_df[common_columns]])\n","\n","groups = {k: pd.DataFrame(v) for k, v in common_df[common_df.epoch == 1].groupby([\"frame_size\",\t\"frame_step\",\t\"fft_size\",\t\"mel_bins\"], as_index=False)}\n","groups"],"execution_count":null,"outputs":[]}]}