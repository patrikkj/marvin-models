{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"model-naive.ipynb","provenance":[],"collapsed_sections":["yTd_i9v56NAB","TmjrsRGd4soU","fp_v3dDTX9fM","Z0xEXMTa0bpn","dKBuyErAtczt","B2sPU8sdppj4","8FWj6aZZirUD","E1-hVE75P-Xn"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yTd_i9v56NAB"},"source":["##### Load imports and modules"]},{"cell_type":"code","metadata":{"id":"Hper9R0A5jlG"},"source":["params = {\n","    # Dataset parameters\n","    \"dataset\": \"dataset_half_notrim_tensors\",\n","    \"train_split\": 0.6,\n","    \"val_split\": 0.2,\n","    \"test_split\": 0.2,\n","    \n","    # Preprocessing parameters\n","    'sample_rate': 16_000, \n","    'min_freq': 0, \n","    'max_freq': 8_000\n","}\n","\n","MODEL_TYPE = \"rnn_naive\"\n","LOG_DIR = f\"/content/marvin-models/logs/{MODEL_TYPE}\"\n","LOG_LEVEL = \"ERROR\"\n","GH_TOKEN = \"73a1d93fa1e7fe7696321a86ff037a0ecc58346c\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQtIbvWVxkM0","cellView":"form","executionInfo":{"status":"ok","timestamp":1605411687388,"user_tz":-60,"elapsed":9801,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"08149532902010492583"}},"outputId":"92c69483-5100-46d0-e3c9-e247c4805fa4","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@markdown <b>Run me to import underscore module</b><br/>   {display-mode: \"form\"}\n","#@markdown <small>Method signatures:</small><br/> \n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _under(source_path, target_path, copy=True, auth_on_upload=True)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _set_gh_token(token)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _from_gh(user_name, repo_name, release_name) &nbsp; &nbsp; &nbsp; <b>Returns:</B> dictionary of arrays { 'array_name' : np.ndarray }</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs)</small></small><br/>\n","#@markdown <small><small>&nbsp; &nbsp; &nbsp; _export_model(model, model_name, model_type, val_dataset, test_dataset, params, hparams, history, log_dir, n_prep_layers=None)</small></small><br/>\n","!pip install -q tensorflowjs\n","!pip install -q githubrelease\n","import numpy as np\n","import os, glob, re, time, json\n","import github_release\n","import tensorflow.keras.backend as K\n","from contextlib import redirect_stdout\n","\n","compressed_dirs = set()\n","\n","\n","def _compress(source_path, target_path, target_dir=None):\n","    if target_dir:\n","        !mkdir -p {target_dir}\n","    if target_path.endswith('.tar.gz'):\n","        !tar -czf {target_path} -C {source_path} .\n","    elif target_path.endswith('.tar'):\n","        !tar -cf {target_path} -C {source_path} .\n","    elif target_path.endswith('.zip'):\n","        !(cd {source_path} && zip -q -r {target_path} .)\n","\n","\n","def _extract(source_path, target_path):\n","    !mkdir -p {target_path}\n","    if source_path.endswith('.tar.gz'):\n","        !tar -xzf {source_path} -C {target_path}\n","    elif source_path.endswith('.tar'):\n","        !tar -xf {source_path} -C {target_path}\n","    elif source_path.endswith('.zip'):\n","        !unzip -qq {source_path} -d {target_path}\n","\n","\n","def _under(source_path, target_path, copy=True, auth_on_upload=True):\n","    \"\"\"\n","    Use cases:\n","        Movement:\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Compression (e.g. from dir to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction (e.g. from .zip to dir):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","            \n","        Extraction & compression (e.g. from .zip to .tar.gz):\n","            - GCS -> GCS\n","            - GCS -> LOCAL\n","            - LOCAL -> GCS\n","            - LOCAL -> LOCAL\n","    \"\"\"\n","    COMPRESSION_FORMATS = ('zip', 'tar', 'tar.gz')\n","    TEMP_DIR = \"/tmp_\"\n","    LOG_TEMPLATE = \"{}    from    {}    to    {}\"\n","\n","    # Source\n","    if source_path.endswith(\"/\"):\n","        source_path = source_path[:-1]\n","    source_dir, _, source_name = source_path.rpartition('/')\n","    source_isgcs = source_path.startswith(\"gs://\")\n","    source_islocal = not source_isgcs\n","    if source_islocal:\n","        source_path = os.path.abspath(source_path)\n","    source_isprefix, source_isfile, source_ext = source_name.partition('.')\n","    source_isdir = not source_isfile\n","    source_iscompression = source_ext in COMPRESSION_FORMATS\n","\n","    # Target\n","    target_dir, _, target_name = target_path.rpartition('/')\n","    target_isgcs = target_path.startswith(\"gs://\")\n","    target_islocal = not target_isgcs\n","    target_prefix, target_isfile, target_ext = target_name.partition('.')\n","    target_isdir = not target_isfile\n","    target_iscompression = target_ext in COMPRESSION_FORMATS\n","\n","    # Flags\n","    MOVE_ONLY = source_ext == target_ext\n","    GCS_ONLY = source_isgcs and target_isgcs\n","    RENAME = source_isprefix != target_prefix\n","    COMPRESSION = source_isdir and target_iscompression\n","    EXTRACTION = source_iscompression and target_isdir\n","    EXTRACTION_COMPRESSION = source_iscompression and target_iscompression and source_ext != target_ext\n","\n","    # Add forward slash if file is at root level\n","    source_dir = \"/\" if not source_dir else source_dir\n","    target_dir = \"/\" if not target_dir else target_dir\n","\n","    # Authenticate if writing to GCS\n","    if target_isgcs and auth_on_upload:\n","        from google.colab import auth\n","        auth.authenticate_user()\n","\n","    # Assert that subdirectories exist if target is local\n","    if target_islocal:\n","        !mkdir -p {target_dir}\n","\n","    # Movement commands\n","    if MOVE_ONLY:\n","        # GCS -> GCS\n","        if source_isgcs and target_isgcs:\n","            action = \"COPYING\" if copy else \"MOVING\"\n","            print(LOG_TEMPLATE.format(f\"{action} (1/1)\", source_path, target_path))\n","            if copy:\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                !gsutil -m -q mv {source_path} {target_path}\n","        \n","        # LOCAL -> LOCAL\n","        elif source_islocal and target_islocal:\n","            action = \"COPYING\" if copy else \"MOVING\"\n","            print(LOG_TEMPLATE.format(f\"{action} (1/1)\", source_path, target_path))\n","            if copy:\n","                !cp -r {source_path} {target_path}\n","            else:\n","                !mv {source_path} {target_path}\n","        \n","        # GCS -> LOCAL\n","        elif source_isgcs and target_islocal:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING DIR (1/1)\", source_path, target_dir))\n","                !gsutil -m -q cp -r {source_path} {target_dir}\n","                if RENAME:\n","                    print(LOG_TEMPLATE.format(\"\\tRENAMING DIR\", source_isprefix, target_prefix))\n","                    !mv {target_dir}/{source_isprefix} {target_dir}/{target_prefix}\n","            else:\n","                print(LOG_TEMPLATE.format(\"DOWNLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        \n","        # LOCAL -> GCS\n","        if source_islocal and target_isgcs:\n","            if source_isdir:\n","                print(LOG_TEMPLATE.format(\"UPLOADING DIR (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp -r {source_path} {target_path}\n","            else:\n","                print(LOG_TEMPLATE.format(\"UPLOADING FILE (1/1)\", source_path, target_path))\n","                !gsutil -m -q cp {source_path} {target_path}\n","        return\n","\n","\n","    # Create directory for intermediate storage if required\n","    if source_isgcs or target_isgcs or EXTRACTION_COMPRESSION:\n","        !mkdir -p {TEMP_DIR}\n","    \n","\n","    # For remaining operations, download GCS source to temp and treat as local\n","    if source_isgcs:\n","        if source_isdir:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING DIR\", source_path, TEMP_DIR))\n","            !gsutil -m -q cp -r {source_path} {TEMP_DIR}\n","        else:\n","            print(LOG_TEMPLATE.format(\"\\tDOWNLOADING FILE\", source_path, f\"{TEMP_DIR}/{source_name}\"))\n","            !gsutil -m -q cp {source_path} {TEMP_DIR}/{source_name}\n","        source_path = f\"{TEMP_DIR}/{source_name}\"\n","        source_dir = TEMP_DIR\n","\n","    # Compression\n","    if COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/1)\", source_path, target_path))\n","            _compress(source_path, target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction\n","    elif EXTRACTION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/1)\", source_path, target_path))\n","            _extract(source_path, target_path)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_name}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING DIR (2/2)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp -r {TEMP_DIR}/{target_name} {target_path}\n","\n","    # Extraction & compression\n","    elif EXTRACTION_COMPRESSION:\n","        if target_islocal:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/2)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/2)\", f\"{TEMP_DIR}/{target_prefix}\", target_path))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", target_path, target_dir=target_dir)\n","        else:\n","            print(LOG_TEMPLATE.format(\"EXTRACTING (1/3)\", source_path, f\"{TEMP_DIR}/{target_prefix}\"))\n","            _extract(source_path, f\"{TEMP_DIR}/{target_prefix}\")\n","            print(LOG_TEMPLATE.format(\"COMPRESSING (2/3)\", f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\"))\n","            _compress(f\"{TEMP_DIR}/{target_prefix}\", f\"{TEMP_DIR}/{target_name}\")\n","            print(LOG_TEMPLATE.format(\"UPLOADING FILE (3/3)\", f\"{TEMP_DIR}/{target_name}\", target_path))\n","            !gsutil -m -q cp {TEMP_DIR}/{target_name} {target_path}\n","    \n","    # Cleanup intermediate storage\n","    !rm -rf {TEMP_DIR}\n","\n","def _set_gh_token(token):\n","    os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","def _export_array(array, release_name, prefix=\"\", splits=3):\n","    dir_path = f\"/tmp_/{release_name}\"\n","    !mkdir -p {dir_path}\n","    n_digits = len(str(splits - 1))\n","    subarrays = np.array_split(array, splits)\n","    for i, subarray in enumerate(subarrays):\n","        filename = f\"{prefix}__{str(i).zfill(n_digits)}.npy\"\n","        np.save(f\"{dir_path}/{filename}\", subarray)\n","\n","\n","def _concat_arrays(paths):\n","    return np.concatenate([np.load(path, allow_pickle=True) for path in sorted(paths)])\n","\n","\n","def _to_gh(user_name, repo_name, release_name, split_size=600, **arr_kwargs):\n","    # Assert that GitHub Auth token is set\n","    if \"GITHUB_TOKEN\" not in os.environ:\n","        print(\"GitHub authentication token is not set.\")\n","        print(\"Set token using the '_set_gh_token(token_string)' method.\")\n","        print(\"Minimal required auth scope is 'repo/public_repo' for public repositories.\")\n","        print(\"URL: https://github.com/settings/tokens/new\")\n","        return\n","\n","    # Split arrays\n","    for prefix, array in arr_kwargs.items():\n","        splits = int((array.nbytes/1_000_000) // split_size) + 1\n","        _export_array(array, release_name, prefix=prefix, splits=splits)\n","\n","    # Upload arrays\n","    github_release.gh_release_create(\n","        f\"{user_name}/{repo_name}\", \n","        release_name, \n","        publish=True, \n","        name=release_name, \n","        asset_pattern=f\"/tmp_/{release_name}/*\"\n","    )\n","    !rm -rf /tmp_/*\n","\n","\n","def _from_gh(user_name, repo_name, release_name):\n","    # Download release to temporary directory\n","    print(\"Downloading dataset in parallell ... \", end='\\t')\n","    t0 = time.perf_counter()\n","    assets = github_release.get_assets(f\"{user_name}/{repo_name}\", tag_name=release_name)\n","    download_urls = [asset['browser_download_url'] for asset in assets]\n","    urls_str = \" \".join(download_urls)\n","    !echo {urls_str} | xargs -n 1 -P 8 wget -q -P /tmp_/{release_name}_dl/\n","    t1 = time.perf_counter()\n","    print(f\"done! ({t1 - t0:.3f} seconds)\")\n","\n","    # Load data into numpy arrays\n","    paths = glob.glob(f\"/tmp_/{release_name}_dl/*.npy\")\n","    groups = {}\n","    for path in paths:\n","        match = re.match(r\".*/(.*)__[0-9]*\\.npy\", path)\n","        if match:\n","            prefix = match.group(1)\n","            groups[prefix] = groups.get(prefix, []) + [path]\n","    arrays_dict = {name: _concat_arrays(paths) for name, paths in groups.items()}\n","    !rm -rf /tmp_/*\n","    return arrays_dict\n","    \n","\n","def _log_to_gh(user, repo, tag, log_dir=\"/tmp/logs\"):\n","    # Create temporary directory for compressed logs\n","    !mkdir -p /tmp/compressed_logs\n","    \n","    # Compress all directories in log dir\n","    for dirname in os.listdir(log_dir):\n","        # Skip files\n","        if \".\" in dirname or dirname in compressed_dirs:\n","            continue\n","\n","        # Compress\n","        _under(f\"{log_dir}/{dirname}\", f\"/tmp/compressed_logs/{dirname}.tar.gz\")\n","        compressed_dirs.add(dirname)\n","\n","    # Upload compressed logs to GitHub\n","    github_release.gh_asset_upload(f\"{user}/{repo}\", tag, f\"/tmp/compressed_logs/*.tar.gz\")\n","\n","    # Cleanup compressed logs\n","    !rm -rf /tmp/compressed_logs/*\n","\n","def timeit(method):\n","    def timed(*args, **kw):\n","        ts = time.perf_counter()\n","        result = method(*args, **kw)\n","        te = time.perf_counter()\n","        diff = te - ts\n","        print(f\"{method.__name__}: {diff:.8f} s\")\n","        return result\n","    return timed\n","\n","class NpEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, np.integer):\n","            return int(obj)\n","        elif isinstance(obj, np.floating):\n","            return float(obj)\n","        elif isinstance(obj, np.ndarray):\n","            return obj.tolist()\n","        else:\n","            return super(NpEncoder, self).default(obj)\n","\n","@timeit\n","def _export_model(model, model_name, model_type, train_dataset, val_dataset, test_dataset, params, hparams, history, log_dir, n_prep_layers=None):\n","    # Create temporary directory\n","    target_dir = f\"/tmp/models/{model_type}/{model_name}\"\n","    !mkdir -p {target_dir}     #/tmp_/models/rnn_naive/rnn_naive_20201108_130308\n","\n","    # Write export logs to file\n","    export_logs_path = os.path.join(target_dir, \"export_logs.txt\")\n","    with open(export_logs_path, 'w') as export_logs:\n","        with redirect_stdout(export_logs):\n","            # Profile model on inputs (average of n_runs cycles)\n","            n_runs = 5\n","            input_shape = val_dataset.element_spec[0].shape\n","\n","            def time_examples(model, n):\n","                dummy = np.random.rand(n, *input_shape[1:])\n","                t0 = time.perf_counter()\n","                model.predict(dummy)\n","                return time.perf_counter() - t0\n","\n","            with tf.device('/CPU:0'):\n","                cpu_profiles = {\n","                    \"cpu_1\": np.array([time_examples(model, 1) for _ in range(n_runs)]).mean(),\n","                    \"cpu_10\": np.array([time_examples(model, 10) for _ in range(n_runs)]).mean(),\n","                    \"cpu_100\": np.array([time_examples(model, 100) for _ in range(n_runs)]).mean()\n","                }\n","\n","            with tf.device('/GPU:0'):\n","                gpu_profiles = {\n","                    \"gpu_1\": np.array([time_examples(model, 1) for _ in range(n_runs)]).mean(),\n","                    \"gpu_10\": np.array([time_examples(model, 10) for _ in range(n_runs)]).mean(),\n","                    \"gpu_100\": np.array([time_examples(model, 100) for _ in range(n_runs)]).mean()\n","                }\n","\n","            # Get number of parameters\n","            params_counts = {\n","                \"trainable_params\": np.sum([K.count_params(w) for w in model.trainable_weights]),\n","                \"non_trainable_params\": np.sum([K.count_params(w) for w in model.non_trainable_weights])\n","            }\n","            params_counts[\"total_params\"] = params_counts[\"trainable_params\"] + params_counts[\"non_trainable_params\"]\n","\n","            # Generate evaluation metrics for validation and test set\n","            # print(\"BEFORE EVALUATE\")\n","            # final_metrics_train = model.evaluate(train_dataset, return_dict=True)\n","            # final_metrics_train = {f\"final_train_{k}\": v for k, v in final_metrics_train.items()}\n","            # print(\"AFTER EVALUATE\")\n","            final_metrics_val = model.evaluate(val_dataset, return_dict=True)\n","            final_metrics_val = {f\"final_val_{k}\": v for k, v in final_metrics_val.items()}\n","            final_metrics_test = model.evaluate(test_dataset, return_dict=True)\n","            final_metrics_test = {f\"final_test_{k}\": v for k, v in final_metrics_test.items()}\n","\n","            # Generate Dataframe and export to parquet\n","            logs_params = {\n","                \"num_epochs\": len(history.epoch),\n","                **params,\n","                **hparams,\n","                **history.params,\n","                **cpu_profiles,\n","                **gpu_profiles,\n","                **params_counts,\n","                # **final_metrics_train,\n","                **final_metrics_val,\n","                **final_metrics_test\n","            }\n","            logs_df = pd.DataFrame({**history.history, \"epoch\": history.epoch})\n","            for param, value in logs_params.items():\n","                logs_df[param] = value\n","            logs_df.to_parquet(os.path.join(target_dir, f\"{model_name}.parquet\"))\n","\n","            # Dump all parameters and metadata to .json file\n","            with open(os.path.join(target_dir, 'model_details.json'), 'w') as f:\n","                json.dump(logs_params, f, cls=NpEncoder, indent=4)\n","\n","            def _convert_model(model, subdir=\"model\"):\n","                # Create subdirectory\n","                subdir_path = os.path.join(target_dir, subdir)\n","                !mkdir -p {subdir_path}\n","\n","                # Write model summary to file\n","                model_summary_path = os.path.join(subdir_path, \"model_summary.txt\")\n","                with open(model_summary_path, 'w') as model_summary:\n","                    with redirect_stdout(model_summary):\n","                        model.summary()\n","\n","                # Export model summary as image\n","                model_summary_img_path = os.path.join(subdir_path, \"model_summary.png\")\n","                tf.keras.utils.plot_model(model, to_file=model_summary_img_path, show_shapes=True)\n","\n","                # Generate model paths\n","                keras_model_path = os.path.join(subdir_path, \"keras_model.h5\")\n","                saved_model_path = os.path.join(subdir_path, \"saved_model\")\n","                tfjs_layers_model_path = os.path.join(subdir_path, \"tfjs_layers_model\")\n","                tfjs_graph_model_path = os.path.join(subdir_path, \"tfjs_graph_model\")\n","\n","                # Save and convert model\n","                model.save(keras_model_path)\n","                tf.saved_model.save(model, saved_model_path)\n","                !tensorflowjs_converter --input_format=keras --output_format=tfjs_layers_model {keras_model_path} {tfjs_layers_model_path}\n","                !tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model {saved_model_path} {tfjs_graph_model_path}\n","            \n","            # Convert full model\n","            _convert_model(model, subdir=\"model\")\n","\n","            if n_prep_layers is not None:\n","                model_1 = tf.keras.Sequential(model.layers[:n_prep_layers])\n","                model_1.build(input_shape=input_shape)\n","                \n","                model_2 = tf.keras.Sequential(model.layers[n_prep_layers:])\n","                model_2.build(input_shape=model_1.layers[-1].output_shape)\n","\n","                # Convert models\n","                _convert_model(model_1, subdir=\"submodel_1\")\n","                _convert_model(model_2, subdir=\"submodel_2\")\n","\n","            # Compress TensorBoard logs\n","            model_log_dir = os.path.join(LOG_DIR, model_name)\n","            tensorboard_logs_path = os.path.join(target_dir, f\"{model_name}.tar.gz\")\n","            _under(model_log_dir, tensorboard_logs_path)\n","\n","    # Upload logs to GCS\n","    _under(target_dir, f\"gs://marvin-voice/models/{model_type}/{model_name}\", auth_on_upload=False)\n","    return logs_df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                          | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                     | 20kB 31.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 30kB 24.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà           | 40kB 27.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 51kB 26.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 61kB 28.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 9.6MB/s \n","\u001b[?25h\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñè                            | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 20kB 33.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 40kB 25.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                | 51kB 21.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 61kB 20.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè         | 71kB 19.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç      | 81kB 20.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 92kB 19.2MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 102kB 20.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112kB 20.5MB/s \n","\u001b[?25h  Building wheel for linkheader (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"both","id":"lApr2AmG3-Gw","executionInfo":{"status":"ok","timestamp":1605411696376,"user_tz":-60,"elapsed":18784,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"08149532902010492583"}},"outputId":"3399e8a0-ca5d-4707-bc64-a3d73da2c1df","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Download git repository\n","import os\n","if not os.getcwd().endswith(\"marvin-models\"):\n","    !git config --global user.email \"patrikkja@gmail.com\"\n","    !git config --global user.name \"Patrik Kj√¶rran\"\n","    !git clone -q https://github.com/patrikkj/marvin-models.git\n","    %cd marvin-models\n","\n","# Internal modules\n","import io, sys, glob, time\n","from datetime import datetime\n","from importlib import reload\n","\n","# External modules\n","!pip install -q pydub\n","!pip install -q tensorflow-io\n","#!pip install -q -U tensorboard_plugin_profile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorflow_io as tfio\n","import tensorflow_addons as tfa\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, LearningRateScheduler\n","from tensorboard.plugins.hparams import api as hp\n","\n","# Colab modules\n","from google.colab import auth\n","from IPython import display\n","    \n","# Scripts (record_audio requires pydub installed)\n","import scripts\n","\n","# Set random number generation seeds\n","np.random.seed(1)\n","tf.random.set_seed(1)\n","\n","# Enable logging to GitHub release tag\n","_set_gh_token(GH_TOKEN)\n","\n","# Set logging level\n","!mkdir -p {LOG_DIR}\n","tf.get_logger().setLevel(LOG_LEVEL)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/marvin-models\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22.4MB 1.4MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TmjrsRGd4soU"},"source":["##### Download dataset\n"]},{"cell_type":"code","metadata":{"cellView":"both","id":"wkUPW24VxvwH","executionInfo":{"status":"ok","timestamp":1605411725111,"user_tz":-60,"elapsed":47515,"user":{"displayName":"Patrik Kj√¶rran","photoUrl":"","userId":"08149532902010492583"}},"outputId":"04d15e51-3630-4e74-9be4-0594f004c793","colab":{"base_uri":"https://localhost:8080/"}},"source":["if 'arrays' not in globals():\n","    arrays = _from_gh(\"patrikkj\", \"marvin-models\", params['dataset'])\n","    pos_data, pos_labels = arrays['pos_data'], arrays['pos_labels']\n","    neg_data, neg_labels = arrays['neg_data'], arrays['neg_labels']\n","\n","    # Dataset characteristics\n","    classes = np.array([0, 1])\n","    split = [params['train_split'], params['val_split'], params['test_split']]\n","\n","    # Build dataset\n","    arrays, info = scripts.datasets.build_dataset(pos_data, pos_labels, neg_data, neg_labels, split)\n","    TRAIN_SIZE, VAL_SIZE, TEST_SIZE = info\n","    POS_SIZE = pos_data.shape[0]\n","    NEG_SIZE = neg_data.shape[0]\n","    TOTAL_SIZE = POS_SIZE + NEG_SIZE"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading dataset in parallell ... \tdone! (27.072 seconds)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fp_v3dDTX9fM"},"source":["##### Build TensorFlow graph\n"]},{"cell_type":"code","metadata":{"cellView":"both","id":"xvKxVTrGdrOc"},"source":["cell_types = {\n","    'gru': tf.keras.layers.GRU,\n","    'lstm': tf.keras.layers.LSTM,\n","    'bi-gru': lambda *args, **kwargs: tf.keras.layers.Bidirectional(tf.keras.layers.GRU(*args, **kwargs)),\n","    'bi-lstm': lambda *args, **kwargs: tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(*args, **kwargs)),\n","}\n","\n","optimizers = {\n","    'adam': tf.keras.optimizers.Adam,\n","    'rmsprop': tf.keras.optimizers.RMSprop,\n","    'sgd': tf.keras.optimizers.SGD\n","}\n","\n","def build_model(params, hparams, output_bias):\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Input(shape=(params['sample_rate'],), dtype=tf.float32))\n","    model.add(tf.keras.layers.Reshape((int(params['sample_rate']/hparams['n_timesteps']), hparams['n_timesteps'])))\n","\n","    # Convolutional units\n","    model.add(tf.keras.layers.Conv1D(hparams['conv_filters'], hparams['conv_kernel_size'], strides=hparams['conv_stride'], activation='relu'))\n","\n","    # Recurrent units\n","    for _ in range(hparams['recurrent_layers'] - 1):\n","        model.add(cell_types[hparams['recurrent_cell']](hparams['recurrent_units'], input_shape=(params['sample_rate'],), return_sequences=True))\n","    model.add(cell_types[hparams['recurrent_cell']](hparams['recurrent_units']))\n","    model.add(tf.keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias))\n","    return model\n","\n","def train_model(params, hparams, metrics, log_dir, model_type, output_bias=None, **fit_kwargs):\n","    # Prepare output bias for imbalanced distributions\n","    if output_bias is not None:\n","        output_bias = None if output_bias == 0 else tf.keras.initializers.Constant(output_bias)\n","\n","    # Create callbacks and prepare logging\n","    early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n","    timestamp = datetime.now()\n","    dir_name = f\"{model_type}_{timestamp:%Y%m%d_%H%M%S}\"\n","    filename = f\"{log_dir}/{dir_name}\"\n","    hparams[\"__timestamp__\"] = int(f\"{timestamp:%Y%m%d%H%M%S}\")\n","    tensorboard = TensorBoard(filename, write_graph=False, histogram_freq=0, write_images=False) # profile_batch='50,70',\n","    hp_board = hp.KerasCallback(filename, hparams, trial_id=dir_name)\n","    callbacks = [tensorboard, hp_board, early_stopping]\n","\n","    # Build model and run\n","    model = build_model(params, hparams, output_bias)\n","    model.compile(optimizers[hparams['optimizer']](learning_rate=hparams['learning_rate']), 'binary_crossentropy', metrics)\n","    history = model.fit(callbacks=callbacks, **fit_kwargs)\n","    return model, history, dir_name"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0xEXMTa0bpn"},"source":["##### Define hyperparameter domains"]},{"cell_type":"code","metadata":{"cellView":"both","id":"OauJScx--Y9g"},"source":["# Evaluation metrics\n","metrics = [tf.keras.metrics.Precision(name='precision'),\n","      tf.keras.metrics.Recall(name='recall'),\n","      tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n","      \n","hp_metrics = [hp.Metric('accuracy', display_name='Accuracy'),\n","    hp.Metric('precision', display_name='Precision'),\n","    hp.Metric('recall', display_name='Recall')]\n","\n","# Set hyperparameter domains\n","hparams_refs = {\n","    'pos_weight': hp.HParam('pos_weight', hp.Discrete([POS_SIZE/TOTAL_SIZE, 0.1, 0.2, 0.3, 0.4, 0.5])),\n","    'n_timesteps': hp.HParam('n_timesteps', hp.Discrete([4, 5, 8, 10, 16, 32, 64])),\n","\n","    'conv_filters': hp.HParam('conv_filters', hp.Discrete([32, 64])),\n","    'conv_kernel_size': hp.HParam('conv_kernel_size', hp.Discrete([3, 5, 15, 24])),\n","    'conv_stride': hp.HParam('conv_stride', hp.Discrete([1, 2, 3, 4])),\n","  \n","    # 'recurrent_cell': hp.HParam('recurrent_cell', hp.Discrete(['gru', 'lstm', 'bi-gru', 'bi-lstm'])),\n","    'recurrent_cell': hp.HParam('recurrent_cell', hp.Discrete(['gru', 'bi-gru'])),\n","    'recurrent_layers': hp.HParam('recurrent_layers', hp.IntInterval(1, 4)),\n","    'recurrent_units': hp.HParam('recurrent_units', hp.Discrete([32, 64, 128])),\n","\n","    'batch_size': hp.HParam('batch_size', hp.Discrete([64, 128, 256])),\n","    'optimizer': hp.HParam('optimizer', hp.Discrete(['adam', 'rmsprop', 'sgd'])),\n","    'learning_rate': hp.HParam('learning_rate', hp.Discrete([10**-3.5, 10e-4])),\n","}\n","\n","# Hyperparameter constraints\n","constraints = {\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dKBuyErAtczt"},"source":["##### Random search"]},{"cell_type":"code","metadata":{"id":"iIKGsUGhwkVg","outputId":"9adad45a-53c5-4a12-f49d-dba2fdbf607a","colab":{"base_uri":"https://localhost:8080/"}},"source":["#@title &nbsp;\n","# Random serach with parameter lock\n","hparams_locked = {\n","    'pos_weight': 0.1,\n","    # 'n_timesteps': 16,\n","\n","    # 'conv_kernel_size': 3,\n","    # 'conv_stride': 1,\n","    # 'conv_filters': 32,\n","    \n","    # 'recurrent_cell': 'gru',\n","    # 'recurrent_layers': 3,\n","    # 'recurrent_units': 64,\n","\n","    # 'batch_size': 128,\n","    'optimizer': 'adam',\n","    # 'learning_rate': 0.001,\n","}\n","\n","NUM_ITERATIONS = 10000\n","for i in range(NUM_ITERATIONS):\n","    hparams = {k: v.domain.sample_uniform() for k, v in hparams_refs.items() if k not in hparams_locked}\n","    hparams.update(hparams_locked)\n","\n","    # Assert that hyperparameter allocation is valid\n","    while True:\n","        for hparam, hparam_constraints in constraints.items():\n","            # Reallocate invalid hyperparameters\n","            if not all(constraint(hparams) for constraint in hparam_constraints):\n","                hparams[hparam] = hparams_refs[hparam].domain.sample_uniform()\n","                break\n","        else: # If all allocations are valid, break out of while loop\n","            break\n","\n","    try:\n","        # Build datasets\n","        datasets = scripts.datasets.resample_dataset(arrays, hparams['batch_size'], hparams['pos_weight'])\n","        train_dataset, val_dataset, test_dataset = datasets\n","        output_bias = np.log([hparams['pos_weight']/(1-hparams['pos_weight'])])\n","        print(hparams)\n","\n","        fit_kwargs = {\n","            \"x\": train_dataset,\n","            \"validation_data\": val_dataset,\n","            \"epochs\": 100,\n","            \"steps_per_epoch\": TRAIN_SIZE // hparams[\"batch_size\"],\n","            \"verbose\": 0\n","        }\n","        model, history, model_name = train_model(params, hparams, metrics, log_dir=LOG_DIR, model_type=MODEL_TYPE, output_bias=output_bias, **fit_kwargs)\n","        _export_model(model, model_name, MODEL_TYPE, train_dataset,val_dataset, test_dataset, params, hparams, history, LOG_DIR)\n","    except Exception as e:\n","        print(e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'n_timesteps': 64, 'conv_filters': 32, 'conv_kernel_size': 5, 'conv_stride': 3, 'recurrent_cell': 'bi-gru', 'recurrent_layers': 1, 'recurrent_units': 128, 'batch_size': 64, 'learning_rate': 0.00031622776601683794, 'pos_weight': 0.1, 'optimizer': 'adam'}\n","UPLOADING DIR (1/1)    from    /tmp/models/rnn_naive/rnn_naive_20201115_034210    to    gs://marvin-voice/models/rnn_naive/rnn_naive_20201115_034210\n","_export_model: 26.97888854 s\n","{'n_timesteps': 10, 'conv_filters': 32, 'conv_kernel_size': 5, 'conv_stride': 4, 'recurrent_cell': 'gru', 'recurrent_layers': 3, 'recurrent_units': 32, 'batch_size': 64, 'learning_rate': 0.00031622776601683794, 'pos_weight': 0.1, 'optimizer': 'adam'}\n","UPLOADING DIR (1/1)    from    /tmp/models/rnn_naive/rnn_naive_20201115_034709    to    gs://marvin-voice/models/rnn_naive/rnn_naive_20201115_034709\n","_export_model: 27.70602568 s\n","{'n_timesteps': 4, 'conv_filters': 64, 'conv_kernel_size': 15, 'conv_stride': 2, 'recurrent_cell': 'bi-gru', 'recurrent_layers': 2, 'recurrent_units': 32, 'batch_size': 64, 'learning_rate': 0.00031622776601683794, 'pos_weight': 0.1, 'optimizer': 'adam'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B2sPU8sdppj4"},"source":["##### Single iteration https://bit.ly/34Z2pDM"]},{"cell_type":"code","metadata":{"cellView":"both","id":"4xZbZItE6Oej"},"source":["# Set hyperparameters\n","hparams = {\n","    'pos_weight': 0.5, \n","\n","    'conv_kernel_size': 3,\n","    'conv_stride': 1,\n","    'conv_filters': 32,\n","    'n_timesteps': 16,\n","    \n","    'recurrent_cell': 'gru',\n","    'recurrent_layers': 3,\n","    'recurrent_units': 64,\n","\n","    'batch_size': 128,\n","    'optimizer': 'adam',\n","    'learning_rate': 0.001,\n","}\n","\n","# Build datasets\n","datasets = scripts.datasets.resample_dataset(arrays, hparams['batch_size'], hparams['pos_weight'])\n","train_dataset, val_dataset, test_dataset = datasets\n","output_bias = np.log([hparams['pos_weight']/(1-hparams['pos_weight'])])\n","\n","fit_kwargs = {\n","    \"x\": train_dataset,\n","    \"validation_data\": val_dataset,\n","    \"epochs\": 3,\n","    \"steps_per_epoch\": TRAIN_SIZE // hparams[\"batch_size\"],\n","    \"verbose\": 1\n","}\n","model, history, model_name = train_model(params, hparams, metrics, log_dir=LOG_DIR, model_type=MODEL_TYPE, output_bias=output_bias, **fit_kwargs)\n","_export_model(model, model_name, MODEL_TYPE, val_dataset, test_dataset, params, hparams, history, LOG_DIR, n_prep_layers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8FWj6aZZirUD"},"source":["##### Test model on custom input and visualize"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Xxhd7kXblNWo"},"source":["#@title Marvin! &#128039;&#128039;&#128039;\n","duration = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n","audio = scripts.record_audio.record(duration)\n","desired_sample_rate = 16_000\n","sample_rate = audio.frame_rate\n","raw_audio = audio.raw_data\n","stride_size = desired_sample_rate//5\n","threshold = 0.5\n","\n","tensor = tf.io.decode_raw(audio.raw_data, tf.int32, fixed_length=sample_rate*duration*audio.sample_width)\n","tensor = tf.cast(tensor, tf.float32)\n","tensor /= 32768.0**2\n","tensor = tfio.audio.resample(tensor, sample_rate, desired_sample_rate)\n","\n","# Generate sliding window dataset\n","audio_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n","    data=tensor, \n","    targets=None, \n","    sequence_length=desired_sample_rate, \n","    sequence_stride=stride_size, \n","    batch_size=1)\n","\n","# Model prediction\n","predictions = model.predict(audio_dataset)\n","is_marvins = predictions > threshold\n","clip_length = tensor.shape[0]\n","n_segments = predictions.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"WJE7JeDolNO7"},"source":["#@title Plot entire waveform\n","def set_size(w,h, ax):\n","    if not ax: ax=plt.gca()\n","    l = ax.figure.subplotpars.left\n","    r = ax.figure.subplotpars.right\n","    t = ax.figure.subplotpars.top\n","    b = ax.figure.subplotpars.bottom\n","    figw = float(w)/(r-l)\n","    figh = float(h)/(t-b)\n","    ax.figure.set_size_inches(figw, figh)\n","\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(24,3))\n","\n","# Create upper graph\n","ax1.plot(tensor.numpy(), color='white')\n","ax1.axis('off')\n","\n","# Create bubbles\n","x_shift = stride_size//2\n","x = np.linspace(x_shift, clip_length - stride_size//2, num=n_segments)\n","y = np.ones(n_segments)\n","colors = predictions\n","area = 100 + is_marvins * 500\n","num = 1\n","ax2.scatter(x, y, s=area, c=colors, alpha=0.9, cmap='RdYlGn')\n","ax2.axis('off')\n","set_size(24, 1, ax=ax2)\n","\n","# Plot figure\n","fig.tight_layout(rect=[0, 0, 1, 1])\n","plt.show()\n","plt.ioff()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"8vCRB08U_5b8"},"source":["#@title Plot segments\n","\n","\n","# Plot figure\n","FIG_WIDTH = 24\n","\n","fig, axes = plt.subplots(n_segments + 1, 2, figsize=(FIG_WIDTH, (n_segments + 1)*0.5), gridspec_kw={'width_ratios':[1,20]})\n","axes_flat = axes.flat\n","axes[0, 0].axis('off')\n","top_axis = axes[0, 1]\n","top_axis.plot(tensor.numpy(), color='white')\n","top_axis.axis('off')\n","top_axis.set_xbound(0, clip_length)\n","\n","def set_size(w,h, ax):\n","    if not ax: ax=plt.gca()\n","    l = ax.figure.subplotpars.left\n","    r = ax.figure.subplotpars.right\n","    t = ax.figure.subplotpars.top\n","    b = ax.figure.subplotpars.bottom\n","    figw = float(w)/(r-l)\n","    figh = float(h)/(t-b)\n","    ax.figure.set_size_inches(figw, figh)\n","\n","for i, item in enumerate(zip(audio_dataset, predictions)):\n","    audio, prediction = item\n","    dot_ax, ax = axes_flat[2*(i+1):2*(i+2)]\n","\n","    # Fetch data\n","    is_marvin = prediction > 0.5\n","    audio_data = audio.numpy().squeeze()\n","    x = np.arange(i*stride_size, i*stride_size + desired_sample_rate)\n","\n","    # Create bubble\n","    colors = predictions\n","    area = 500 #100 + is_marvins * 500\n","    dot_ax.scatter([0], [0], s=area, c=[prediction], alpha=0.9, cmap='RdYlGn', vmin=0, vmax=1, )\n","    dot_ax.set_xlim(-1, 1)\n","    dot_ax.axis('off')\n","    #set_size(1, 1, ax=dot_ax)\n","\n","    # Plot segment\n","    #set_size(FIG_WIDTH, 1*n_segments+1, ax)\n","    ax.plot(x, audio_data, color='green' if is_marvin else 'grey')\n","    ax.set_xbound(0, clip_length)\n","    ax.axis('off')\n","    ax.get_shared_x_axes().join(ax, top_axis)\n","fig.tight_layout(rect=[0, 0, 1, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"0Y45twpo8AVS"},"source":["#@title  Plot detailed audio pairs\n","def set_size(w,h, ax):\n","    if not ax: ax=plt.gca()\n","    l = ax.figure.subplotpars.left\n","    r = ax.figure.subplotpars.right\n","    t = ax.figure.subplotpars.top\n","    b = ax.figure.subplotpars.bottom\n","    figw = float(w)/(r-l)\n","    figh = float(h)/(t-b)\n","    ax.figure.set_size_inches(figw, figh)\n","\n","x_shift = stride_size//2\n","x = np.linspace(x_shift, clip_length - stride_size//2, num=n_segments)\n","y = np.ones(n_segments)\n","colors = predictions\n","area = 100 + is_marvins * 500\n","\n","fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(24,3))\n","\n","ax1.plot(tensor.numpy(), color='white')\n","ax1.axis('off')\n","\n","ax2.scatter(x, y, s=area, c=colors, alpha=0.9, cmap='RdYlGn')\n","ax2.axis('off')\n","set_size(24, 1, ax=ax2)\n","\n","fig.tight_layout(rect=[0, 0, 1, 1])\n","plt.show()\n","plt.ioff()\n","\n","# Visualize outputs\n","for i, item in enumerate(zip(audio_dataset, predictions)):\n","    audio, prediction = item\n","    is_marvin = prediction > 0.5\n","    audio_data = audio.numpy().squeeze()\n","\n","    displays = []\n","    #displays.append(f\"{str(is_marvin).capitalize()}\")\n","    displays.append(display.Audio(audio_data, rate=16_000))\n","\n","    buf = io.BytesIO()\n","    fig = plt.figure(figsize=(8,1.2))\n","    plt.plot(audio_data, color='green' if is_marvin else 'grey')\n","    plt.axis('off')\n","    fig.tight_layout(rect=[0, 0, 1, 1])\n","    plt.savefig(buf, format='png', transparent=True)\n","    plt.close(fig)\n","    buf.seek(0)\n","\n","    displays.append(display.Image(buf.read()))\n","    display.display(*displays)\n","    print('\\n')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1-hVE75P-Xn"},"source":["##### TensorBoard"]},{"cell_type":"code","metadata":{"id":"UHwu1cpXLsM7"},"source":["#https://76bdzjdczr36-496ff2e9c6d22116-6006-colab.googleusercontent.com/#scalars\n","import tensorboard as tb\n","%reload_ext tensorboard\n","%tensorboard --logdir logs/rnn --port 6006\n","display.clear_output(wait=False)\n","tb.notebook.display(height=1400)"],"execution_count":null,"outputs":[]}]}